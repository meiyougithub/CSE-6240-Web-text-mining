{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3 - Q1 Xianghui Gu (xgu72, 903248583)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read data from labeledTrainData, testData and unlabeledTrainData\n",
    "- This blog uses labeledTrainData as training set and testData as testing set for submission\n",
    "- There are 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Read data from files \n",
    "train = pd.read_csv( \"labeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv( \"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- save the test label for submission use in the near future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.to_csv('label.csv', index = False, columns = [\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. String data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clean html, non-letter characters, stop words\n",
    "- lower case all letters\n",
    "- split words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    ### to avoid warning, emphasize html parser\n",
    "    review_text = BeautifulSoup(review, \"html.parser\").get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use nltk english tokenizer to convert a sentence into a list of words in the nltk dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download the punkt tokenizer for sentence splitting\n",
    "import nltk.data\n",
    "# nltk.download()   \n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, we use training set (labeled data) only.\n",
    "- In addition, we removed the stop words for consistency as following methods.\n",
    "- After breaking down sentences, we obtain 266551 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xianghui/Desktop/cse6240/cse6240/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "/Users/xianghui/Desktop/cse6240/cse6240/lib/python3.6/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print (\"Parsing sentences from training set\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer, True)\n",
    "\n",
    "# print (\"Parsing sentences from unlabeled set\")\n",
    "# for review in unlabeled_train[\"review\"]:\n",
    "#     sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266551\n"
     ]
    }
   ],
   "source": [
    "print (len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Word to Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 100    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 5          # Context window size                                                                                    \n",
    "# downsampling = 1e-3   # Downsample setting for frequent words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-17 15:53:15,360 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2017-04-17 15:53:15,367 : INFO : collecting all words and their counts\n",
      "2017-04-17 15:53:15,368 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-04-17 15:53:15,400 : INFO : PROGRESS: at sentence #10000, processed 114931 words, keeping 17627 word types\n",
      "2017-04-17 15:53:15,430 : INFO : PROGRESS: at sentence #20000, processed 228988 words, keeping 24797 word types\n",
      "2017-04-17 15:53:15,460 : INFO : PROGRESS: at sentence #30000, processed 339533 words, keeping 29883 word types\n",
      "2017-04-17 15:53:15,490 : INFO : PROGRESS: at sentence #40000, processed 453983 words, keeping 34196 word types\n",
      "2017-04-17 15:53:15,517 : INFO : PROGRESS: at sentence #50000, processed 565006 words, keeping 37609 word types\n",
      "2017-04-17 15:53:15,550 : INFO : PROGRESS: at sentence #60000, processed 676637 words, keeping 40571 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-17 15:53:15,581 : INFO : PROGRESS: at sentence #70000, processed 789005 words, keeping 43180 word types\n",
      "2017-04-17 15:53:15,614 : INFO : PROGRESS: at sentence #80000, processed 899771 words, keeping 45561 word types\n",
      "2017-04-17 15:53:15,648 : INFO : PROGRESS: at sentence #90000, processed 1013453 words, keeping 47982 word types\n",
      "2017-04-17 15:53:15,681 : INFO : PROGRESS: at sentence #100000, processed 1125135 words, keeping 50054 word types\n",
      "2017-04-17 15:53:15,712 : INFO : PROGRESS: at sentence #110000, processed 1236261 words, keeping 51928 word types\n",
      "2017-04-17 15:53:15,747 : INFO : PROGRESS: at sentence #120000, processed 1348541 words, keeping 53966 word types\n",
      "2017-04-17 15:53:15,780 : INFO : PROGRESS: at sentence #130000, processed 1461911 words, keeping 55694 word types\n",
      "2017-04-17 15:53:15,810 : INFO : PROGRESS: at sentence #140000, processed 1568503 words, keeping 57193 word types\n",
      "2017-04-17 15:53:15,846 : INFO : PROGRESS: at sentence #150000, processed 1682622 words, keeping 58902 word types\n",
      "2017-04-17 15:53:15,877 : INFO : PROGRESS: at sentence #160000, processed 1794988 words, keeping 60464 word types\n",
      "2017-04-17 15:53:15,909 : INFO : PROGRESS: at sentence #170000, processed 1907744 words, keeping 61924 word types\n",
      "2017-04-17 15:53:15,944 : INFO : PROGRESS: at sentence #180000, processed 2018412 words, keeping 63343 word types\n",
      "2017-04-17 15:53:15,976 : INFO : PROGRESS: at sentence #190000, processed 2131820 words, keeping 64641 word types\n",
      "2017-04-17 15:53:16,006 : INFO : PROGRESS: at sentence #200000, processed 2245187 words, keeping 65934 word types\n",
      "2017-04-17 15:53:16,039 : INFO : PROGRESS: at sentence #210000, processed 2357380 words, keeping 67237 word types\n",
      "2017-04-17 15:53:16,070 : INFO : PROGRESS: at sentence #220000, processed 2470883 words, keeping 68544 word types\n",
      "2017-04-17 15:53:16,100 : INFO : PROGRESS: at sentence #230000, processed 2582936 words, keeping 69805 word types\n",
      "2017-04-17 15:53:16,133 : INFO : PROGRESS: at sentence #240000, processed 2697681 words, keeping 71014 word types\n",
      "2017-04-17 15:53:16,162 : INFO : PROGRESS: at sentence #250000, processed 2805851 words, keeping 72198 word types\n",
      "2017-04-17 15:53:16,193 : INFO : PROGRESS: at sentence #260000, processed 2916657 words, keeping 73325 word types\n",
      "2017-04-17 15:53:16,215 : INFO : collected 74065 word types from a corpus of 2988095 raw words and 266551 sentences\n",
      "2017-04-17 15:53:16,216 : INFO : Loading a fresh vocabulary\n",
      "2017-04-17 15:53:16,263 : INFO : min_count=40 retains 8160 unique words (11% of original 74065, drops 65905)\n",
      "2017-04-17 15:53:16,264 : INFO : min_count=40 leaves 2627279 word corpus (87% of original 2988095, drops 360816)\n",
      "2017-04-17 15:53:16,286 : INFO : deleting the raw counts dictionary of 74065 items\n",
      "2017-04-17 15:53:16,290 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2017-04-17 15:53:16,291 : INFO : downsampling leaves estimated 2494390 word corpus (94.9% of prior 2627279)\n",
      "2017-04-17 15:53:16,292 : INFO : estimated required memory for 8160 words and 100 dimensions: 10608000 bytes\n",
      "2017-04-17 15:53:16,319 : INFO : resetting layer weights\n",
      "2017-04-17 15:53:16,421 : INFO : training model with 4 workers on 8160 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-04-17 15:53:17,437 : INFO : PROGRESS: at 9.54% examples, 1185608 words/s, in_qsize 7, out_qsize 1\n",
      "2017-04-17 15:53:18,438 : INFO : PROGRESS: at 17.94% examples, 1118474 words/s, in_qsize 7, out_qsize 1\n",
      "2017-04-17 15:53:19,445 : INFO : PROGRESS: at 28.07% examples, 1162078 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-17 15:53:20,453 : INFO : PROGRESS: at 38.28% examples, 1188445 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-17 15:53:21,458 : INFO : PROGRESS: at 47.93% examples, 1189715 words/s, in_qsize 6, out_qsize 1\n",
      "2017-04-17 15:53:22,461 : INFO : PROGRESS: at 56.74% examples, 1174465 words/s, in_qsize 6, out_qsize 1\n",
      "2017-04-17 15:53:23,473 : INFO : PROGRESS: at 65.24% examples, 1155863 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-17 15:53:24,476 : INFO : PROGRESS: at 74.81% examples, 1160183 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-17 15:53:25,481 : INFO : PROGRESS: at 83.21% examples, 1147307 words/s, in_qsize 8, out_qsize 0\n",
      "2017-04-17 15:53:26,482 : INFO : PROGRESS: at 91.68% examples, 1137641 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-17 15:53:27,338 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-17 15:53:27,340 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-17 15:53:27,341 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-17 15:53:27,346 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-17 15:53:27,347 : INFO : training on 14940475 raw words (12471623 effective words) took 10.9s, 1142602 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print (\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save the word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-17 14:29:33,052 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-04-17 14:29:33,129 : INFO : saving Word2Vec object under 100features_40minwords_5context_training, separately None\n",
      "2017-04-17 14:29:33,130 : INFO : not storing attribute syn0norm\n",
      "2017-04-17 14:29:33,131 : INFO : not storing attribute cum_table\n",
      "2017-04-17 14:29:33,231 : INFO : saved 100features_40minwords_5context_training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"100features_40minwords_5context_training\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Numeric Representations of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the model that we created above\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"100features_40minwords_5context_training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get word_vectors (a matrix of word, word features) with shape: (8160, 100)\n",
    "- set the num_clusters to 10 for k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vectors = model.wv.syn0\n",
    "num_clusters = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fit k-means model using word_vectors as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering:  1.377249002456665 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start = time.time() # Start time\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "idx = kmeans_clustering.fit_predict( word_vectors )\n",
    "\n",
    "# Get the end time and print how long the process took\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print (\"Time taken for K Means clustering: \", elapsed, \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- index2word converts the index to real word\n",
    "- create word_centroid_map to record (word, classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number                                                                                            \n",
    "word_centroid_map = dict(zip( model.wv.index2word, idx ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- generate top twenty words closest to its centroid in each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(We made an assumption that the 20 closest points to a centroid of cluster $i$ belongs to cluster $i$. We check if they all belong to cluster $i$ in for loop. If the data violates the assumption, like in the google dataset, we have to extract the points of each cluster first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0\n",
      "['lang', 'brazilian', 'earliest', 'regarded', 'ealing', 'fifties', 'imho', 'landmark', 'manga', 'masterpieces', 'influential', 'kurosawa', 'sf', 'laputa', 'sergio', 'comparable', 'update', 'bakshi', 'acclaim', 'fellini']\n",
      "\n",
      "Cluster 1\n",
      "['ambiance', 'enhance', 'enhanced', 'layered', 'pleasing', 'delicious', 'strained', 'unusually', 'coupled', 'moody', 'understated', 'framing', 'lively', 'tones', 'distinctive', 'maintains', 'mesmerizing', 'precise', 'rendering', 'combining']\n",
      "\n",
      "Cluster 2\n",
      "['client', 'protective', 'scheming', 'frankie', 'warden', 'warns', 'creasy', 'avenge', 'salesman', 'threatened', 'convict', 'preacher', 'rescued', 'politician', 'befriends', 'estranged', 'pursued', 'slave', 'ritual', 'buys']\n",
      "\n",
      "Cluster 3\n",
      "['keith', 'glenda', 'moss', 'wright', 'gilbert', 'plummer', 'lauren', 'neal', 'burgess', 'miranda', 'gloria', 'downey', 'tyler', 'arkin', 'everett', 'kathleen', 'bauer', 'ian', 'thelma', 'armstrong']\n",
      "\n",
      "Cluster 4\n",
      "['sharks', 'racing', 'knocks', 'axe', 'carpet', 'wheel', 'frozen', 'garage', 'severed', 'laying', 'bike', 'sits', 'climbing', 'worm', 'motel', 'brush', 'hairy', 'desk', 'dirt', 'plate']\n",
      "\n",
      "Cluster 5\n",
      "['inclined', 'conclude', 'criticize', 'intend', 'ought', 'warn', 'dismiss', 'dont', 'comprehend', 'predict', 'guarantee', 'sane', 'assuming', 'spoiling', 'ignore', 'badness', 'assure', 'offend', 'offended', 'wont']\n",
      "\n",
      "Cluster 6\n",
      "['taped', 'popped', 'eagerly', 'netflix', 'tcm', 'pm', 'advertised', 'tonight', 'checked', 'theatres', 'vcr', 'tickets', 'premiere', 'december', 'yesterday', 'broadcast', 'channels', 'purchased', 'mart', 'screened']\n",
      "\n",
      "Cluster 7\n",
      "['preaching', 'ambiguity', 'isolation', 'subtext', 'fundamental', 'vital', 'acceptance', 'turmoil', 'significance', 'communication', 'examination', 'psyche', 'inherent', 'determination', 'internal', 'compassion', 'perception', 'consequence', 'symbolic', 'addressed']\n",
      "\n",
      "Cluster 8\n",
      "['russia', 'korea', 'slavery', 'occupied', 'regime', 'egypt', 'alliance', 'cuban', 'atlantic', 'spies', 'islands', 'pacific', 'northern', 'nations', 'ruled', 'corporation', 'colony', 'communist', 'circus', 'presidential']\n",
      "\n",
      "Cluster 9\n",
      "['derivative', 'unnecessarily', 'unimaginative', 'plodding', 'choppy', 'preposterous', 'contrived', 'unoriginal', 'inane', 'formulaic', 'incomprehensible', 'banal', 'overdone', 'unrealistic', 'incoherent', 'melodramatic', 'hokey', 'nonsensical', 'dreary', 'unreal']\n"
     ]
    }
   ],
   "source": [
    "for cluster in range(0,10):\n",
    "    #\n",
    "    # Print the cluster number  \n",
    "    print (\"\\nCluster %d\" % cluster)\n",
    "    #\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words = []\n",
    "    # transform the data to distance to cluster i\n",
    "    dist = kmeans_clustering.transform(word_vectors)[:, cluster]\n",
    "    # return the indicies the smallest 20 distances\n",
    "    close_ind = np.argpartition(dist,20)[:20]\n",
    "    # check assumption made above using word count \n",
    "    word_count = 0\n",
    "    # check the 20 words are clustered in cluster i\n",
    "    # then append to the words list\n",
    "    for ind in close_ind:\n",
    "        word = model.wv.index2word[ind]\n",
    "        # check if the word belongs to the cluster i\n",
    "        if word_centroid_map[word] == cluster:\n",
    "            words.append(word)\n",
    "            word_count += 1\n",
    "    if word_count != 20:\n",
    "        print(\"Cluster is not well-behaved\")\n",
    "    print (words)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- convert a word list (from one sentence) to #counts in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
    "    #\n",
    "    # The number of clusters is equal to the highest cluster index\n",
    "    # in the word / centroid map\n",
    "    num_centroids = max( word_centroid_map.values() ) + 1\n",
    "    #\n",
    "    # Pre-allocate the bag of centroids vector (for speed)\n",
    "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
    "    #\n",
    "    # Loop over the words in the review. If the word is in the vocabulary,\n",
    "    # find which cluster it belongs to, and increment that cluster count \n",
    "    # by one\n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #\n",
    "    # Return the \"bag of centroids\"\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- clean train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_train_reviews = []\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- save the cleaned version of train and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"clean_train_reviews.csv\", 'w') as f:\n",
    "    csv.writer(f).writerows(clean_train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"clean_test_reviews.csv\", 'w') as f:\n",
    "    csv.writer(f).writerows(clean_test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create bag of centroids for both train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "train_centroids = np.zeros( (train[\"review\"].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids( review, \\\n",
    "        word_centroid_map )\n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Repeat for test reviews \n",
    "test_centroids = np.zeros(( test[\"review\"].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter] = create_bag_of_centroids( review, \\\n",
    "        word_centroid_map )\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- save the processed data (# counts in each cluster per record) as X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('X1.out', train_centroids, delimiter=',') \n",
    "np.savetxt('X1_test.out', test_centroids, delimiter=',') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Pre-trained data from Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "google_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The dataset is (3000000, 300), which means there are 3000000 words and 300 features. \n",
    "The size is way too large for simple k-means. Also for comparison with method 1, we use the words intersected with the words in dictionary in method 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8160"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get google words as list\n",
    "google_words = google_model.index2word\n",
    "# get training model words as set\n",
    "training_words = set(model.wv.index2word)\n",
    "numWordsTotal = len(training_words)\n",
    "numWordsTotal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After intersecting with the training word list, there are 7810 words left. The loss might come from difference in stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7810"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection = []\n",
    "intersection_words = []\n",
    "for pos, word in enumerate(google_words):\n",
    "    if word in training_words:\n",
    "        intersection.append(pos)\n",
    "        intersection_words.append(word)\n",
    "    if len(intersection) > numWordsTotal:\n",
    "        break\n",
    "len(intersection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- extract feature vectors of the intersected words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "google_word_vectors = google_model.syn0[intersection]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fit k-means for google dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for K Means clustering:  5.314241886138916 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time() # Start time\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "google_kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
    "google_idx = google_kmeans_clustering.fit_predict( google_word_vectors )\n",
    "\n",
    "# Get the end time and print how long the process took\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print (\"Time taken for K Means clustering: \", elapsed, \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "google_word_centroid_map = dict(zip(intersection_words, google_idx ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- obtain top twenty words to its centroid in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build word list and index list for each cluster\n",
    "# the index is w.r.t. the intersection_words\n",
    "google_cluster_words = [[] for cluster in range(0,10)]\n",
    "google_cluster_word_indices = [[] for cluster in range(0,10)]\n",
    "for new_pos, word in enumerate(intersection_words):\n",
    "    c = google_word_centroid_map[word]\n",
    "    google_cluster_words[c].append(word)\n",
    "    google_cluster_word_indices[c].append(new_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Google Cluster 0\n",
      "['sense', 'philo', 'great', 'phantasm', 'genuinely', 'life', 'genuine', 'surely', 'brilliant', 'truly', 'moment', 'finally', 'true', 'world', 'happily', 'love', 'glorious', 'undeniably', 'mind', 'inspiring']\n",
      "\n",
      "Google Cluster 1\n",
      "['boyer', 'clooney', 'akshay', 'also', 'kidman', 'travolta', 'even', 'sheridan', 'one', 'darth', 'erika', 'actually', 'corbett', 'mgm', 'however', 'though', 'vance', 'although', 'either', 'dalton']\n",
      "\n",
      "Google Cluster 2\n",
      "['claus', 'nicholson', 'gunga', 'sutherland', 'jabba', 'vivian', 'downey', 'jodie', 'marcel', 'mccoy', 'kaufman', 'aniston', 'kirsten', 'locke', 'scarface', 'cunningham', 'atlantis', 'vader', 'nora', 'casper']\n",
      "\n",
      "Google Cluster 3\n",
      "['man', 'old', 'young', 'people', 'mother', 'family', 'father', 'someone', 'person', 'men', 'woman', 'children', 'teenage', 'cousin', 'husband', 'house', 'son', 'city', 'police', 'parents']\n",
      "\n",
      "Google Cluster 4\n",
      "['knows', 'sees', 'comes', 'makes', 'adds', 'thinks', 'stands', 'takes', 'turns', 'puts', 'gets', 'seems', 'works', 'goes', 'likes', 'says', 'looks', 'happens', 'uses', 'finds']\n",
      "\n",
      "Google Cluster 5\n",
      "['seen', 'gone', 'leaving', 'got', 'left', 'set', 'brought', 'used', 'made', 'came', 'referring', 'added', 'called', 'turned', 'went', 'said', 'meant', 'given', 'coming', 'pushed']\n",
      "\n",
      "Google Cluster 6\n",
      "['contemporary', 'quirky', 'film', 'fatale', 'movie', 'whimsical', 'captivating', 'classic', 'style', 'delightfully', 'charming', 'imaginative', 'character', 'pixar', 'hilarious', 'sly', 'delightful', 'wonderfully', 'oddly', 'subtly']\n",
      "\n",
      "Google Cluster 7\n",
      "['armstrong', 'ritchie', 'leonard', 'conrad', 'anita', 'novak', 'connor', 'bridget', 'mccarthy', 'hoffman', 'lindsey', 'fitzgerald', 'judd', 'laurie', 'suzanne', 'baldwin', 'alexandra', 'carla', 'ethan', 'norris']\n",
      "\n",
      "Google Cluster 8\n",
      "['admittedly', 'odd', 'stupid', 'ridiculous', 'sort', 'silly', 'frankly', 'strange', 'predictably', 'horrible', 'terrible', 'alas', 'pseudo', 'absurd', 'somehow', 'nonsensical', 'laughable', 'awful', 'weird', 'sadly']\n",
      "\n",
      "Google Cluster 9\n",
      "['like', 'anyway', 'come', 'make', 'going', 'something', 'work', 'certainly', 'go', 'think', 'see', 'maybe', 'get', 'turn', 'probably', 'know', 'let', 'back', 'help', 'obviously']\n"
     ]
    }
   ],
   "source": [
    "for cluster in range(0,10):\n",
    "    #\n",
    "    # Print the cluster number  \n",
    "    print (\"\\nGoogle Cluster %d\" % cluster)\n",
    "    #\n",
    "    # Find all of the words for that cluster number, and print them out\n",
    "    words = []\n",
    "    # transform the data to distance to cluster i\n",
    "    dist = google_kmeans_clustering.transform(google_word_vectors)[:, cluster]\n",
    "    # pick out the distance of the points in cluster i\n",
    "    dist_in_cluster = dist[np.array(google_cluster_word_indices[cluster],dtype='int64')]\n",
    "    # return the indicies the smallest 20 distances\n",
    "    close_ind = np.argpartition(dist_in_cluster,20)[:20]\n",
    "    # word count set to 0 for checking\n",
    "    word_count = 0\n",
    "    # check the 20 words are clustered in cluster i\n",
    "    # then append to the words list\n",
    "    for ind in close_ind:\n",
    "        word = google_cluster_words[cluster][ind]\n",
    "        if google_word_centroid_map[word] == cluster:\n",
    "            words.append(word)\n",
    "            word_count += 1\n",
    "    if word_count != 20:\n",
    "        print(\"Cluster is not well-behaved\")\n",
    "    print (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
    "google_train_centroids = np.zeros( (train[\"review\"].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "\n",
    "# Transform the training set reviews into bags of centroids\n",
    "counter = 0\n",
    "for review in clean_train_reviews:\n",
    "    google_train_centroids[counter] = create_bag_of_centroids( review, \\\n",
    "        google_word_centroid_map )\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Repeat for test reviews \n",
    "google_test_centroids = np.zeros(( test[\"review\"].size, num_clusters), \\\n",
    "    dtype=\"float32\" )\n",
    "\n",
    "counter = 0\n",
    "for review in clean_test_reviews:\n",
    "    google_test_centroids[counter] = create_bag_of_centroids( review, \\\n",
    "        google_word_centroid_map )\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('X2.out', google_train_centroids, delimiter=',') \n",
    "np.savetxt('X2_test.out', google_test_centroids, delimiter=',') "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "cse6240",
   "language": "python",
   "name": "cse6240"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
